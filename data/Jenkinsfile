pipeline {
    agent { label 'docker' }

    parameters {
        string(
            name: 'FORCE_DATA_HASH',
            defaultValue: '',
            description: 'Use given archived DATA_HASH instead of ' +
                'processing a new one'
        )
        string(
            name: 'FORCE_TIMESTAMP',
            defaultValue: '',
            description: 'Use given archived TIMESTAMP instead of ' +
                'processing a new one'
        )
    }

    environment {
        FORCE_DATA_HASH = "${params.FORCE_DATA_HASH}"
        FORCE_TIMESTAMP = "${params.FORCE_TIMESTAMP}"
        ADJUST_USER_UID = sh(
            returnStdout: true,
            script: 'id -u'
        ).trim()
        ADJUST_USER_GID = sh(
            returnStdout: true,
            script: 'id -g'
        ).trim()
        ADJUST_DOCKER_GID = sh(
            returnStdout: true,
            script: 'getent group docker | cut -d: -f3'
        ).trim()
    }

    stages {
        stage('Process') {
            agent {
                docker {
                    alwaysPull true
                    image 'kuralabs/python3-dev:latest'
                    args '-u root:root'
                }
            }

            steps {
                sh '''
                sudo --user=python3 --preserve-env --set-home ./data/process.sh
                '''
                stash name: 'data', includes: 'ws/**/*'
            }
        }

        stage('Finalize') {

            steps {
                unstash 'data'

                script {
                    def latest = readJSON file: 'ws/latest.json'

                    docker.image('mysql:8.0').withRun(
                        "-e MYSQL_ALLOW_EMPTY_PASSWORD=yes"
                    ) { container ->

                        docker.image('mysql:8.0').inside(
                            "--link ${container.id}:db"
                        ) {
                            withEnv([
                                "DATA_HASH=${latest.sha256}",
                                "TIMESTAMP=${latest.timestamp}",
                            ]) {
                                sh '''
                                ./data/finalize.sh
                                '''
                            }
                        }
                    }
                }

                stash name: 'data', includes: 'ws/**/*'
            }
        }

        stage('Publish') {
            agent { label 'archive' }
            steps {
                unstash 'data'
                sh '''#!/usr/bin/env bash
                set -o errexit
                set -o nounset
                set -o xtrace

                umask 022
                mkdir -p "/deploy/archive/mivotico/tse2sql/$(date +%Y)"
                cp --no-clobber --recursive ws/* "/deploy/archive/mivotico/tse2sql/$(date +%Y)"
                '''
            }
        }
    }
    post {
        success {
            slackSend (
                color: '#00FF00',
                message: ":sunny: SUCCESSFUL: " +
                    "<${env.BUILD_URL}|[${env.BUILD_NUMBER}] ${env.JOB_NAME}>"
            )
        }

        failure {
            slackSend (
                color: '#FF0000',
                message: ":rain_cloud: FAILED: " +
                    "<${env.BUILD_URL}|[${env.BUILD_NUMBER}] ${env.JOB_NAME}>"
            )
        }
    }
}
