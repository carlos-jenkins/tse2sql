pipeline {
    agent none

    stages {
        stage('Process') {

            agent {
                docker {
                    image 'kuralabs/python3-dev:latest'
                }
            }

            steps {
                sh '''#!/usr/bin/env bash

                set -o errexit
                set -o nounset
                set -o xtrace

                echo "Installing tse2sql ..."
                tox -e build
                pip3 install dist/tse2sql-*.whl

                mkdir ws
                pushd ws

                echo "===== CSV DATA SOURCE PROCESSING ====="

                mkdir csv
                pushd csv

                SOURCE_URL=https://www.tse.go.cr/zip/padron/padron_completo.zip
                echo "Downloading currently published electoral roll from ${SOURCE_URL} ..."
                curl --output padron.zip "${SOURCE_URL}"

                echo "Calculating data hash ..."
                DATA_HASH=$(sha256sum --binary padron.zip | cut -d' ' -f 1)
                echo "Data hash : ${DATA_HASH}"

                mkdir "${DATA_HASH}"
                pushd "${DATA_HASH}"

                echo "Checking archive for ${DATA_HASH} ..."
                ARCHIVE_URL="https://archive.kuralabs.io/mivotico/tse2sql/$(date +%Y)/csv/${DATA_HASH}/${DATA_HASH}.tar.gz"
                DATA_AVAILABLE=$(curl --silent --head "${ARCHIVE_URL}" | grep "404 Not Found" || true)

                if [ -z "${DATA_AVAILABLE}" ]; then

                    echo "Data source ${DATA_HASH} already processed. Will use archived results."

                    echo "Removing source data ..."
                    rm ../padron.zip

                    echo "Downloading archived results from ${ARCHIVE_URL} ..."
                    curl --output "${DATA_HASH}.tar.gz" "${ARCHIVE_URL}"

                    echo "Extracting archived results from ${DATA_HASH}.tar.gz ..."
                    tar -zxvf "${DATA_HASH}.tar.gz"

                else

                    echo "New data source ${DATA_HASH}. Processing ..."

                    mv ../padron.zip "${DATA_HASH}.zip"
                    tse2sql "${DATA_HASH}.zip"

                    echo "Compressing generated data for archival ..."
                    tar -zcvf \
                        "${DATA_HASH}.tar.gz" \
                        "${DATA_HASH}.mysql.sql" \
                        "${DATA_HASH}.data.json" \
                        "${DATA_HASH}.samples.json"

                    echo "Removing source data ..."
                    rm -r "${DATA_HASH}" "${DATA_HASH}.zip"
                fi

                popd
                popd

                echo "===== WEB DATA SOURCE PROCESSING ====="

                mkdir web
                pushd web

                TIMESTAMP=$(date +%s)
                echo "Timestamp identifier for this session : ${TIMESTAMP} ..."

                mkdir "${TIMESTAMP}"
                pushd "${TIMESTAMP}"

                echo "Using ${DATA_HASH}.samples.json for this session. Renaming ..."
                cp "../../csv/${DATA_HASH}/${DATA_HASH}.samples.json" "${TIMESTAMP}.samples.json"

                echo "Starting scrapper using ${DATA_HASH}.samples.json ..."
                tse2sql-scrapper "${TIMESTAMP}.samples.json"

                echo "Recording input identifiers ..."
                echo "{\\"sha256\\": \\"${DATA_HASH}\\", \\"timestamp\\": \\"${TIMESTAMP}\\"}" | python3 -m json.tool > "${TIMESTAMP}.input.json"

                echo "Compressing generated data for archival ..."
                tar -zcvf \
                    "${TIMESTAMP}.tar.gz" \
                    "${TIMESTAMP}.input.json" \
                    "${TIMESTAMP}.unscrapped.json" \
                    "${TIMESTAMP}.scrapped.mysql.sql"

                echo "Removing samples file ..."
                rm "${TIMESTAMP}.samples.json"

                popd
                popd

                echo "===== PREPARING DATA ARCHIVE ====="

                echo "Tree before cleaning up ..."
                tree

                echo "Cleaning up workspace ..."
                mv "web/${TIMESTAMP}/${TIMESTAMP}.input.json" latest.json

                rm "csv/${DATA_HASH}/${DATA_HASH}.data.json"
                rm "csv/${DATA_HASH}/${DATA_HASH}.samples.json"
                rm "csv/${DATA_HASH}/${DATA_HASH}.mysql.sql"

                rm "web/${TIMESTAMP}/${TIMESTAMP}.unscrapped.json"
                rm "web/${TIMESTAMP}/${TIMESTAMP}.scrapped.mysql.sql"


                echo "Final data for archiving:"
                tree

                popd
                '''
                stash name: 'data', includes: 'ws/**/*'
            }
        }

        stage('Publish') {
            agent { label 'archive' }
            steps {
                unstash 'data'
                sh '''#!/usr/bin/env bash

                    set -o errexit
                    set -o nounset
                    set -o xtrace

                    umask 022
                    mkdir -p "/deploy/archive/mivotico/tse2sql/$(date +%Y)"
                    cp --no-clobber --recursive ws/* "/deploy/archive/mivotico/tse2sql/$(date +%Y)"
                '''
            }
        }
    }
    post {
        success {
            slackSend (
                color: '#00FF00',
                message: "SUCCESSFUL: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})"
            )
        }

        failure {
            slackSend (
                color: '#FF0000',
                message: "FAILED: Job '${env.JOB_NAME} [${env.BUILD_NUMBER}]' (${env.BUILD_URL})"
            )
        }
    }
}
